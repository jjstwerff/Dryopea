struct SKeyword {
    name: text
};

struct Possible {
    length: integer,
    token: text
};

struct SToken {
    start: character, // the first character of each allowed token for example '<' or '.'
    possible: sorted<Possible[-length,token]> // full tokens, may be longer than 2
};

// The last scanned thing
enum Scanned {
    Unknown,
    Integer, Float, Double, Long,
    Text, Partial, Character,
    Identifier,
    Token, Keyword,
    Finished
}

pub struct Lexer {
    file: text, // we can switch files
    data: text,
    index: integer,
    line: integer,
    pos: integer,
    start: integer, // the start of the last encountered token (till index)
    scanned: Scanned, // the type of the last scanned thing or token
    string: boolean, // we are parsing a string content that contains expressions
    finished: boolean, // is the last parsed string finished
    keywords: hash<SKeyword[name]>,
    tokens: hash<SToken[start]>,
    behind: boolean, // was the last comment behind code
    comment: text // the last encountered comment, dismissed when new code is parsed
}

// Only within a single file can we remember anchor points and return to them later.
pub struct Anchor {
    index: integer,
    line: integer,
    pos: integer
}

// Set keywords, these will never be identifiers.
pub fn set_keywords(self: Lexer, keywords: vector<text>) {
    self.keywords = [];
    for k in keywords {
        self.keywords += [ {name:k} ];
    }
}

// Set tokens, this allows for correct multi-character token parsing.
pub fn set_tokens(self: Lexer, tokens: vector<text>) {
    self.tokens = [];
    for t in tokens {
        f = t[0];
        if !f {
            continue;
        }
        if self.tokens[f] {
            self.tokens[f].possible += [ { length: t.len(), token: t } ];
        } else {
            self.tokens += [ { start: f, possible: [ { length: t.len(), token: t } ] } ];
        }
    }
}

// Skip possible white-space, comments or newlines in the file.
fn skip_whitespace(self: Lexer) {
    self.pos += self.index - self.start;
    start = self.pos == 1;
    for t in 0..300 {
        if self.data[self.index] == ' ' or self.data[self.index] == '\t' {
            self.index += 1;
            self.pos += 1;
        } else if self.data[self.index] == '\n' {
            self.index += 1;
            self.line += 1;
            self.pos = 1;
            start = true;
            println("new_line start {start} pos:{self.pos}");
        } else if self.data[self.index..self.index+2] == "//" {
            println("comment start {start} pos:{self.pos}");
            if !start {
                self.behind = true;
            }
            self.index += 2;
            com = self.index;
            self.pos += 1;
            for c in 0..1000 {
                if self.data[self.index] == '\n' {
                    if self.comment != "" {
                        self.comment += '\n';
                    }
                    self.comment += self.data[com..self.index].trim_start();
                    break;
                }
                self.index += 1;
                self.pos += 1;
            }
        } else {
            break;
        }
    }
    self.start = self.index;
    self.scanned = Unknown;
}

// Scan for known tokens in the data.
// Only increase self.index while keeping self.start at the first position.
fn scan(self: Lexer) {
    self.comment = "";
    self.behind = false;
    self.skip_whitespace();
    negative = false;
    l = self.data[self.index];
    if l == null {
        self.scanned = Finished;
        return;
    }
    if l == '\'' { // Constant character
        self.index += 1;
        self.scanned = Character;
        for t in 0..20 {
            l = self.data[self.index];
            if l == '\'' { // The end of the character
                self.index += 1;
                return;
            }
            if l == '\\' { // Encountered an escape sequence
                self.index += 1;
                if self.data[self.index] == '\'' { // Skip escaped token
                    self.index += 1;
                }
            } else {
                self.index += 1;
            }
        }
    } else if l == '"' { // Constant string
        self.index += 1;
        self.scanned = Text;
        for t in 0..60000 {
            l = self.data[self.index];
            if l == '"' { // The end of the text
                self.index += 1;
                return;
            }
            if l == '\\' { // Encountered an escape sequence
                self.index += 1;
                n = self.data[self.index];
                if n == '"' || n == '\n' { // Skip escaped token or newline
                    self.index += 1;
                }
            } else {
                self.index += 1;
            }
        }
        return;
    } else if l == '-' { // Negative number
        self.index += 1;
        negative = true;
    }
    if self.data[self.index] >= '0' and self.data[self.index] <= '9' { // Various numbers
        self.scanned = Integer;
        for t in 0..30 {
            l = self.data[self.index];
            if l == '.' or l == 'e' or l == '-' {
                self.scanned = Float;
            } else if l == 'l' {
                self.scanned = Long;
            } else if l == 'x' || l == 'o' || l == 'b' { // allow hex/octal and binary indicators
            } else if l != '_' and (l < '0' or l > '9') {
                break;
            }
            self.index += 1;
        }
        return;
    } else if negative {
        self.index -= 1;
    }
    if l.is_alphabetic() { // Identifiers or keywords
        self.scanned = Identifier;
        for t in 0..300 {
            l = self.data[self.index];
            if !l.is_alphanumeric() {
                break;
            }
            self.index += 1;
        }
        return;
    }
    // Tokens
    self.scanned = Token;
    for pt in self.tokens[l].possible {
        tok = pt.token;
        if self.data[self.index..self.index + tok.len()] == tok {
            self.index += tok.len();
            return;
        }
    }
    self.index += 1;
}

// Open a file for parsing.
pub fn parse(self: Lexer, filename: text) {
    v = filename.rfind("/");
    self.file = if v {
        filename[v + 1..]
    } else {
        filename
    };
    self.data = file(filename).content();
    self.index = 0;
    self.start = 0;
    self.line = 1;
    self.pos = 1;
    self.scan();
}

// Use a text for parsing.
pub fn parse_string(self: Lexer, name: text, content: text) {
    self.file = name;
    self.data = content;
    self.index = 0;
    self.start = 0;
    self.line = 1;
    self.pos = 1;
    self.scan();
}

// Test if there is a specific token, do not skip it.
pub fn test(self: Lexer, with: text) -> boolean {
    self.data[self.start..self.index] == with
}

// Test if there is a specific token or keyword in the file, skip this if it matches.
pub fn matches(self: Lexer, with: text) -> boolean {
    if self.test(with) {
        self.scan();
        true
    } else {
        false
    }
}

// Peek towards the next token, this will skip whitespace.
// Returns unconverted Strings and Character constants.
pub fn peek(self: Lexer) -> text {
    self.data[self.start..self.index]
}

// Gather information about the current lexer position
pub fn position(self: Lexer) -> text {
    "{self.file}:{self.line}:{self.pos}"
}

// Get an identifier and skip it.
pub fn identifier(self: Lexer) -> text {
    result = self.data[self.start..self.index];
    if self.keywords[result] {
        null
    } else {
        self.scan();
        result
    }
}

// Get an integer and skip it.
pub fn int(self: Lexer) -> integer {
    if self.scanned != Integer {
        return null;
    }
    result = self.data[self.start..self.index] as integer;
    if result {
        self.scan();
        result
    } else {
        null
    }
}

// Get a long and skip it.
pub fn long_int(self: Lexer) -> long {
    if self.scanned != Long && self.scanned != Integer {
        return null;
    }
    to = self.index;
    if self.data[to-1] == 'l' {
        to -= 1;
    }
    result = self.data[self.start..to] as long;
    if result {
        self.scan();
        result
    } else {
        null
    }
}

// Get a float
pub fn long_float(self: Lexer) -> float {
    if self.scanned != Float {
        return null;
    }
    result = self.data[self.start..self.index] as float;
    if result {
        self.scan();
        result
    } else {
        null
    }
}

// Get a single length float
pub fn single_float(self: Lexer) -> single {
    if self.scanned != Float {
        return null;
    }
    result = self.data[self.start..self.index] as single;
    if result {
        self.scan();
        result
    } else {
        null
    }
}

pub fn is_finished(self: Lexer) -> boolean {
    self.scanned == Finished
}

fn escaped(self: Lexer, i: integer) -> character {
    c = self.data[i + 1];
    if c == '"' or c == '\'' or c == '\\' or c == '\n' or c == '{' or c == '}' {
        c
    } else if c == 't' {
        '\t'
    } else if c == 'r' {
        '\r'
    } else if c == 'n' {
        '\n'
    } else if c == 'x' { // ascii token
        total = "0" + self.data[i + 1..i + 4];
        nr = total as integer;
        nr as character
    } else if c == 'u' { // not implemented yet unicode token
        0 as character
    } else { // unknown token
        0 as character
    }
}

// TODO allow for partial matching of a text till the next single { token.
pub fn constant_text(self: Lexer) -> text {
    if self.scanned != Text {
        return null;
    }
    result = "";
    for i in self.start+1..self.index-1 {
        if self.data[i] == '\\' {
            result += self.escaped(i);
        } else {
            result += self.data[i];
        }
    }
    self.scan();
    result
}

// TODO validate if more than one character or escape sequence is given
pub fn constant_character(self: Lexer) -> character {
    if self.scanned != Character {
        return 0 as character;
    }
    result = if self.data[self.start + 1] == '\\' {
        self.escaped(self.start + 2)
    } else {
        self.data[self.start + 1]
    };
    self.scan();
    result
}

// Is the last parsed constant string finished or is it on another formatting expression.
pub fn string_finished(self: Lexer) -> boolean {
    self.finished
}

// Get the last parsed comment, can contain multiple lines.
pub fn last_comment(self: Lexer) -> text {
    self.comment
}

// Was the last found comment started on the same line as code.
pub fn comment_behind(self: Lexer) -> boolean {
    self.behind
}

pub fn anchor(self: Lexer) -> Anchor {
    Anchor { index: self.index, line: self.line, pos: self.pos }
}

pub fn revert(self: Lexer, to: Anchor) {
    self.index = to.index;
    self.line = to.line;
    self.pos = to.pos;
    self.scan();
}
