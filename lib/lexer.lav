struct Keyword {
    name: text
};

struct Possible {
    length: integer,
    token: text
};

struct Token {
    start: integer, // the first character of each allowed token for example '<' or '.'
    possible: sorted<Possible[-length,token]> // full tokens, may be longer than 2
};

pub struct Lexer {
    file: text, // we can switch files
    data: text,
    index: integer,
    line: integer,
    pos: integer,
    string: boolean, // we are parsing a string content that contains expressions
    finished: boolean, // is the last parsed string finished
    keywords: hash<Keyword[name]>,
    tokens: hash<Token[start]>,
    behind: boolean, // was the last comment behind code
    comment: text // the last encountered comment, dismissed when new code is parsed
}

// Only within a single file can we remember anchor points and return to them later.
pub struct Anchor {
    index: integer,
    line: integer,
    pos: integer
}

// Apply keywords, there will not be identifiers.
pub fn set_keywords(self: Lexer, keywords: vector<text>) {
    self.keywords = [];
    for k in keywords {
        self.keywords += [ {name:k} ];
    }
}

// Set tokens, this allows for correct multi-character token parsing.
pub fn set_tokens(self: Lexer, tokens: vector<text>) {
    self.tokens = [];
    for t in tokens {
        f = t.character();
        if !f {
            continue;
        }
        if self.tokens[f] {
            self.tokens[f].possible += [ { length: t.len(), token: t } ];
        } else {
            self.tokens += [ { start: f, possible: [ { length: t.len(), token: t } ] } ];
        }
    }
}

// Open a file for parsing.
pub fn parse(self: Lexer, filename: text) {
    v = filename.rfind("/");
    self.file = if v {
        filename[v + 1..]
    } else {
        filename
    };
    self.data = file(filename).content();
    self.index = -1;
    self.line = -1;
    self.pos = -1;
}

// Use a text for parsing.
pub fn parse_string(self: Lexer, name: text, content: text) {
    self.file = name;
    self.data = content;
    self.index = -1;
    self.line = -1;
    self.pos = -1;
}

// Skip possible white-space, comments or newlines in the file.
fn skip_whitespace(self: Lexer) {
}

// Test if there is a specific token, do not skip it.
pub fn test(self: Lexer, with: text) -> boolean {
    self.skip_whitespace();
    false
}

// Test if there is a specific token or keyword in the file, skip this if it matches.
pub fn match(self: Lexer, with: text) -> boolean {
    if self.test(with) {
        self.pos += len(with);
        self.behind = false;
        self.comment = "";
        true
    } else {
        false
    }
}

// Peek towards the next token, this will skip whitespace
pub fn peek(self: Lexer) -> text {
    ""
}

// Gather information about the current lexer position
pub fn position(self: Lexer) -> text {
    "{self.file}:{self.line}:{self.pos}"
}

// Get an identifier and skip it.
pub fn identifier(self: Lexer) -> text {
}

// Get an integer and skip it.
pub fn int(self: Lexer) -> integer {
    0
}

// Get a long and skip it.
pub fn long_int(self: Lexer) -> long {
}

// Get a float
pub fn long_float(self: Lexer) -> float {
}

// Get a single length float
pub fn single_float(self: Lexer) -> single {
}

pub fn constant_string(self: Lexer) -> text {
}

// Is the last parsed constant string finished or is it on another formatting expression.
pub fn string_finished(self: Lexer) -> boolean {
    self.finished
}

// Get the last parsed comment, can contain multiple lines.
pub fn last_comment(self: Lexer) -> text {
    self.comment
}

// Was the last found comment started on the same line as code.
pub fn comment_behind(self: Lexer) -> boolean {
    self.behind
}

pub fn anchor(self: Lexer) -> Anchor {
    Anchor { index: self.index, line: self.line, pos: self.pos }
}

pub fn revert(self: Lexer, to: Anchor) {
    self.index = to.index;
    self.line = to.line;
    self.pos = to.pos;
}
